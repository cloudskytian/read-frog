---
title: Ollama
description: 陪读蛙可以借助本地部署的 Ollama 服务器，来实现本地 AI 离线翻译功能，完全免费使用强大的大语言 AI 模型
---

## Ollama 服务器是什么？

Ollama 服务器是一款主打轻量化、便捷性的开源 AI 模型运行平台，让用户无需复杂配置，即可在本地轻松部署各类大语言模型（LLMs）与 AI 模型。它就像一个 “AI 模型管家”，将繁琐的模型下载、配置、运行流程简化为几条命令，哪怕是 AI 部署新手，也能快速上手。

## Ollama 本地部署教程

### 下载安装程序

你可以从 [ollama.ai](https://ollama.ai/) 官网下载安装程序。

### 配置 ollama 的跨域支持

请配置以下的环境变量：

- 允许跨域访问：`OLLAMA_ORIGINS=*`
- API服务监听地址：`OLLAMA_HOST=127.0.0.1:11434` （这是默认地址，可以不用手动配置，不过你可以根据实际情况修改）

## Ollama 常用命令

### 1 检查Ollama版本并且启动ollama服务器

```bash
ollama version
ollama serve
```

用于检查当前安装的 Ollama 版本，以确认安装成功并查看版本详细信息，然后启动 Ollama 服务器

### 2 找到可用模型并安装

你可以从 [ollama.ai](https://ollama.ai/) 找到可用模型并安装。

```bash
ollama pull [模型名称]
```

比如安装 `gemma3:4b` 模型：

```bash
ollama pull gemma3:4b
```

### 3 列出已下载的模型

```bash
ollama list
```

此命令会显示所有本地下载的模型，包括名称、大小和创建时间，以帮助用户管理本地模型资源。

### 4 启动服务

```bash
ollama serve
```

### 5 插件中配置 Ollama 提供商

在插件的设置中，配置 Ollama 提供商，并选择一个模型。

![Ollama 提供商](/tutorial/providers/ollama/ollama.png)

你可以点击“测试链接”按钮，来测试是否连接成功。

## 注意事项

- 不要启动 Ollama Chat 图形客户端，它会和 `ollama serve` 争抢端口。
- 在插件的设置中，配置 Ollama 提供商时，baseURL 默认是 `http://127.0.0.1:11434`，如果你的 `OLLAMA_HOST` 是这个地址，则可以不用手动配置。